{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Neural Networks can be constructed in various ways. We can represent a Neural Network with a directed graph $\\mathcal{G}$. However, as these graphs get more sophisticated, gradient descent-based optimization technics become more challenging. Therefore, there is a need for an automatic gradient calculation through these graphs. \n",
    "\n",
    "We can calculate the Jacobian of a function with respect to network parameters. There are two ways of calculating Jacobians, namely forward-mode and reverse-mode differentiation. Although they give the same result, depending on the structure of the function that we differentiate, one of them can be more efficient. For example, if the size of the range of a function is less than the size of its domain, reverse-mode is more efficient. For more details please read [this](https://www.wikiwand.com/en/Automatic_differentiation).\n",
    "\n",
    "Since, in Deep Learning, we would like to differentiate loss functions, which have scalar outputs, we prefer to use reverse-mode differentiation. **Backpropagation** is a special case of reverse-mode differentiation.\n",
    "\n",
    "## Autograd\n",
    "\n",
    "In this homework, we will implement a NumPy-based automatic differentiation library called ```autograd```. Before moving into the implementation, make sure that you read [chapter 6](https://www.deeplearningbook.org/contents/mlp.html).\n",
    "\n",
    "First, we start with building a graph. Second, we differentiate a node in the graph with respect to all its inputs.\n",
    "\n",
    "### Array\n",
    "\n",
    "In ```autograd```, we call our basic data-structure ```Array```. An array works similar to NumPy ndarray but it is differentiable. In order to run reverse-mode differentiation, array objects keep track of the computational graph to which they belong. Example graphs are shown in the below figure. In singleton graph, array Z contains the graph $\\mathcal{G}$ which has 3 nodes $X$, $Y$, and $Z$. \n",
    "\n",
    "<img src=\"comp-graphs.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "> Figure: Blue circles denote leaf nodes, while the red circle denotes the root node. Gray nodes represent intermediate nodes. Note that non-leaf nodes are the outputs of some operations.\n",
    "\n",
    "In order to understand how array objects are built let's observe the following usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Autograd array, Operation: SimpleAdd: [[1.31601182 1.06189737 1.5497095  1.04361216 1.34210663]\n",
       "  [1.0057547  1.90576253 1.2104615  1.19405544 1.80914202]\n",
       "  [1.73363448 1.71204617 1.66306854 1.40599005 1.1679411 ]\n",
       "  [1.87475525 1.9481673  1.93464611 1.09157235 1.01255101]],\n",
       " Autograd array: [[1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1. 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from autograd.array import Array\n",
    "from autograd.operations import SimpleAdd\n",
    "\n",
    "# Construction of Leaf nodes\n",
    "data = np.random.rand(4, 5)\n",
    "first_array = Array(value=data)\n",
    "second_array = Array(value=np.ones((4, 5)))\n",
    "\n",
    "# Creation of a non-leaf node\n",
    "simple_add_op = SimpleAdd(first_array, second_array)\n",
    "result_array = Array(simple_add_op(), simple_add_op)\n",
    "# Note that, we will not explicitly call operations! This is just for demonstration!\n",
    "\n",
    "assert first_array.operation is None\n",
    "assert second_array.operation is None\n",
    "assert result_array.operation is not None\n",
    "\n",
    "\n",
    "result_array, second_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An array can be constructed in two different ways.\n",
    "- Feeding only the value parameter by a NumPy array\n",
    "- Also by feeding the operation that created the array value\n",
    "\n",
    "If we create an array using the former method, it becomes a leaf node in the computation graph that contains it. Otherwise, it becomes an intermediate or the root node. For example, in the figure above, the simple graph has 4 nodes. Among them, $X$ and $Y$ are leaf nodes (created without feeding operation parameter), $Z$ is the root node, and $T$ is the intermediate node. Both $T$ and $Z$ are created as outputs of addition operations, and hence, they are not leaf nodes. Non-leaf nodes contain the operation that resulted in their creation.\n",
    "\n",
    "> Note: Array object and array value (NumPy Array) are made immutable in order to leave less room for errors in gradient calculations.\n",
    "\n",
    "### Operation\n",
    "\n",
    "Automatic differentiation requires both the forward computation and a derivative of every differentiable operation. Therefore, we define new NumPy-based operations that contain forward and derivative operations. Please take a look at the definition of Operations in the ```operations.py``` script.\n",
    "\n",
    "- call method\n",
    "- jvp method\n",
    "\n",
    "#### Forward computation\n",
    "\n",
    "We implement forward computation in ```__call__``` method. This is trivial since we can directly use NumPy operations.\n",
    "\n",
    "#### Derivative computation\n",
    "\n",
    "In automatic differentiation, we can calculate the Jacobian of a function. However, in Deep Learning, we want to compute gradients of a function with scalar output $g: \\mathcal{R}^n \\rightarrow \\mathcal{R}^1$ with respect to learnable parameters. Hence, instead of computing costly Jacobians, we use vector Jacobian product ```vjp``` to calculate gradients. \n",
    "\n",
    "Suppose that we have a composition of functions $g(x) = f_n\\circ \\dots \\circ f_1(x)$ where $f_i: \\mathcal{R}^n \\rightarrow \\mathcal{R}^n, \\forall i \\in \\{1, \\dots,n-1\\}$ and $f_n: \\mathcal{R}^n \\rightarrow \\mathcal{R}^1$. We calculate the gradient of $g(x)$ as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{J}_x g(x) &= (\\mathcal{J} f_n)(\\mathcal{J} f_{n-1}) \\dots (\\mathcal{J} f_1(x))\\\\\n",
    "    &= (\\nabla^T f_n) (\\mathcal{J} f_{n-1}) \\dots (\\mathcal{J} f_1(x))\\\\\n",
    "    &= \\underbrace{(\\nabla^T (f_n \\circ f_{n-1}))}_{(\\nabla^T f_n) (\\mathcal{J} f_{n-1}):=\\text{vjp}} \\dots (\\mathcal{J} f_1(x))\\\\\n",
    "    &= \\underbrace{(\\nabla^T (f_n \\circ f_{n-1} \\circ f_{n-2}))}_{\\text{vector}^T} \\underbrace{(\\mathcal{J} f_{n-3}(x))\\dots (\\mathcal{J} f_1(x))}_{\\text{Jacobian}}\\\\\n",
    "    &= \\nabla^T_x g(x)\n",
    "\\end{align}\n",
    "\n",
    "Following the above steps, we can calculate vector Jacobian products (vjp) at every step in the computational graph without ever computing and storing the full Jacobian.\n",
    "\n",
    "### Computational Graph\n",
    "\n",
    "Reverse-mode differentiation requires a computational graph $\\mathcal{G}$ of a root node $Z$ to compute its derivative with respect to leaf nodes in the graph. Therefore, we feed the operation object to non-leaf arrays which contain a list of arrays/operands that are used in the operation creation. We call this list $\\mathcal{C}_Z$; children of $Z$. For example in simple-graph, $\\mathcal{C}_Z$ contains $T$ and $Y$. We can traverse the computational-graph $\\mathcal{G}_z$ of root node $Z$ by using the $\\mathcal{C}_Z$ and then recursively traversing their children and so on.\n",
    "\n",
    "We differentiate a node in a computational graph by reversing the directed edges and traversing the reversed graph starting from the root node (the one that we want to differentiate). At every step, we calculate ```vjp```s and pass it to the next node(s) in the reversed graph. Example reversed graph is shown below.\n",
    "\n",
    "<img src=\"comp-graph-backprop.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "> In the reversed graph, the numbers on the reversed directed edges denote the order of which we calculate the vjp operation.\n",
    "\n",
    "Notice that, in the above-reversed graph, in order to evaluate vjp of node $T$ we need to complete vjp calculations of nodes before it ($K$, $L$, and $Z$). The order in which we compute vjps is as follows:\n",
    "- If a node has parents, it must come after all of its parents in the reversed graph\n",
    "\n",
    "This order is important for efficient gradient calculation in reverse-mode differentiation.\n",
    "\n",
    "Luckily, we have an algorithm just as we defined called [topological sorting](https://www.wikiwand.com/en/Topological_sorting#:~:text=In%20computer%20science%2C%20a%20topological,before%20v%20in%20the%20ordering). Please read the link to learn more about it.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd array, Operation: SimpleAdd: [10 25]\n",
      "Autograd array, Operation: SimpleAdd: [3 7]\n",
      "Autograd array: [1 4]\n",
      "Autograd array: [2 3]\n",
      "Autograd array, Operation: SimpleAdd: [ 7 18]\n",
      "Autograd array, Operation: SimpleAdd: [3 7]\n",
      "Autograd array: [1 4]\n",
      "Autograd array: [2 3]\n",
      "Autograd array, Operation: SimpleAdd: [ 4 11]\n",
      "Autograd array, Operation: SimpleAdd: [3 7]\n",
      "Autograd array: [1 4]\n",
      "Autograd array: [2 3]\n",
      "Autograd array: [1 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Autograd array, Operation: SimpleAdd: [10 25]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from autograd.array import Array\n",
    "from autograd.operations import SimpleAdd\n",
    "\n",
    "\n",
    "def add(first_array: Array, second_array: Array) -> Array:\n",
    "    operation = SimpleAdd(first_operand=first_array, second_operand=second_array) # We use this operation just for demonstration!\n",
    "    return Array(value=operation(), operation=operation)\n",
    "\n",
    "\n",
    "arr_x = Array(np.array([1, 4]), is_parameter=True) # Leaf nodes\n",
    "arr_y = Array(np.array([2, 3]), is_parameter=True) # Leaf nodes\n",
    "\n",
    "arr_t = add(arr_x, arr_y) # Intermediate Nodes\n",
    "arr_k = add(arr_t, arr_x) # Intermediate Nodes\n",
    "arr_l = add(arr_t, arr_k) # Intermediate Nodes\n",
    "arr_z = add(arr_t, arr_l) # Root Node\n",
    "\n",
    "def simple_traverse(array: Array) -> None:\n",
    "    print(array)\n",
    "    if array.operation is not None:\n",
    "        for arr in array.operation.operands:\n",
    "            simple_traverse(arr)\n",
    "\n",
    "simple_traverse(arr_z)\n",
    "arr_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We form the computational graph by tracing the operations that we use to create ```Array```s. An example traverse is given above.\n",
    "\n",
    "Now, we want to calculate the derivatives $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial x}$. First, we need to have a topological ordering to start calculating ```jvp```s of the nodes in a proper order.\n",
    "\n",
    "> Complete ```topological_sort``` in the ```__init__.py``` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import topological_sort\n",
    "\n",
    "order = topological_sort(arr_z)\n",
    "for ordered_array, array in zip(order[:-2], (arr_z, arr_l, arr_k, arr_t)):\n",
    "    assert ordered_array.hash_code == array.hash_code, f\"Mismatch between ordered array {ordered_array} and {array}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the gradient using topologically ordered nodes. At every node, accumulate the gradients of each child in the reversed graph. Then use the accumulated gradient to feed it to ```vjp``` method of the array to obtain gradients with respect to its children. For example, in the above figure, in order to calculate vjp of node $T$, we need to accumulate the gradients flowing through $T \\leftarrow K$, $T \\leftarrow L$, $T \\leftarrow Z$ fist.\n",
    "\n",
    "> Complete ```grad``` in the ```__init__.py``` script.\n",
    "\n",
    "Note that, only parameter Arrays (Array objects with ```is_parameter=True```) are returned by ```grad```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENTS defaultdict(<class 'list'>, {Autograd array, Operation: SimpleAdd: [10 25]: [array([1., 1.])]})\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "\n",
    "gradients = grad(arr_z, np.ones(2))\n",
    "assert np.allclose(gradients[arr_x], 4.0), \"Gradient mismatch\"\n",
    "assert np.allclose(gradients[arr_y], 3.0), \"Gradient mismatch\"\n",
    "\n",
    "assert arr_z not in gradients.keys()\n",
    "assert arr_l not in gradients.keys()\n",
    "assert arr_k not in gradients.keys()\n",
    "assert arr_t not in gradients.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that completed, we can fill the remaining operations and complete the Array class. Later we will use it to build Fully Connected Neural Networks and more.\n",
    "\n",
    "### More Operations\n",
    "\n",
    "In order to use addition, subtraction, matrix multiplication, etc we need to override corresponding operators in the Array class. For details please read [Python's data model](https://docs.python.org/3/reference/datamodel.html). We have already filled all of the operators in the Array class. But they use Operations that you need to fill before using these operators.\n",
    "\n",
    "```Python\n",
    "def __add__(self, other: \"Array\") -> \"Array\":\n",
    "    op = Add(self, self.to_array(other))\n",
    "    return Array(op(), operation=op)\n",
    "```\n",
    "\n",
    "In the above code snippet, you can see how we overload the ```+``` operator of Array objects. This operator is called when two arrays, lets say ```A``` and ```B```, are used as ```A + B```.\n",
    "\n",
    "#### Broadcastable Operations\n",
    "\n",
    "```SimpleAdd``` operation that we use to test ```grad```does not automatically broadcast its inputs. Therefore, we need basic arithmetic operations that can do broadcasting and can handle its derivative.\n",
    "\n",
    "> Complete ```BroadcastedOperation``` in the ```operations.py``` script.\n",
    "\n",
    "> Complete ```Add``` in the ```operations.py``` script.\n",
    "\n",
    "> Complete ```Subtract``` in the ```operations.py``` script.\n",
    "\n",
    "> Complete ```Multiply``` in the ```operations.py``` script.\n",
    "\n",
    "> Complete ```Divide``` in the ```operations.py``` script.\n",
    "\n",
    "> Complete ```Maximum``` in the ```operations.py``` script.\n",
    "\n",
    "> Complete ```Minimum``` in the ```operations.py``` script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test ```Add``` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_arr (4, 3, 1)\n",
      "y_arr (5,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from autograd.array import Array\n",
    "\n",
    "x_arr = Array(np.ones((4, 3, 1), dtype=np.float32) * 3)\n",
    "y_arr = Array(np.ones((5,), dtype=np.float32) * 2)\n",
    "\n",
    "z_arr = x_arr + y_arr\n",
    "print(\"x_arr\", x_arr.value.shape)\n",
    "print(\"y_arr\", y_arr.value.shape)\n",
    "\n",
    "assert np.allclose(z_arr.value,  np.ones((4, 3, 5), dtype=np.float32) * 5), \"Forward error in Add\"\n",
    "grad_x, grad_y = z_arr.operation.vjp(np.ones_like(z_arr.value))\n",
    "assert np.allclose(grad_x, np.ones_like(x_arr.value) * 5), \"Derivative error in first argument of Add\"\n",
    "assert np.allclose(grad_y, np.ones_like(y_arr.value) * 12), \"Derivative error in first argument of Add\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Subtract``` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_arr = x_arr - y_arr\n",
    "assert np.allclose(z_arr.value,  np.ones((4, 3, 5), dtype=np.float32)), \"Forward error in Subtract\"\n",
    "grad_x, grad_y = z_arr.operation.vjp(np.ones_like(z_arr.value))\n",
    "assert np.allclose(grad_x, np.ones_like(x_arr.value) * 5), \"Derivative error in first argument of Subtract\"\n",
    "assert np.allclose(grad_y, np.ones_like(y_arr.value) * -12), \"Derivative error in first argument of Subtract\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Multiply``` operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_arr = x_arr * y_arr\n",
    "assert np.allclose(z_arr.value,  np.ones((4, 3, 5), dtype=np.float32) * 6), \"Forward error in Multiply\"\n",
    "grad_x, grad_y = z_arr.operation.vjp(np.ones_like(z_arr.value))\n",
    "assert np.allclose(grad_x, np.ones_like(x_arr.value) * 10), \"Derivative error in first argument of Multiply\"\n",
    "assert np.allclose(grad_y, np.ones_like(y_arr.value) * 36), \"Derivative error in first argument of Multiply\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Divide``` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_arr = x_arr / y_arr\n",
    "assert np.allclose(z_arr.value,  np.ones((4, 3, 5), dtype=np.float32) * 1.5), \"Forward error in Divide\"\n",
    "grad_x, grad_y = z_arr.operation.vjp(np.ones_like(z_arr.value))\n",
    "assert np.allclose(grad_x, np.ones_like(x_arr.value) * 2.5), \"Derivative error in first argument of Divide\"\n",
    "assert np.allclose(grad_y, np.ones_like(y_arr.value) * -9.0), \"Derivative error in first argument of Divide\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Maximum``` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = Array(np.array([[1], [2], [5]], dtype=np.float32))\n",
    "y_arr = Array(np.array([3, 4, -1], dtype=np.float32))\n",
    "\n",
    "z_arr = x_arr.maximum(y_arr)\n",
    "assert np.allclose(z_arr.value, np.array([[3., 4., 1.],\n",
    "                                           [3., 4., 2.],\n",
    "                                           [5., 5., 5.]], dtype=np.float32)), \"Forward error in Maximum\"\n",
    "grad_x, grad_y = z_arr.operation.vjp(np.ones_like(z_arr.value))\n",
    "grad_x, grad_y\n",
    "assert np.allclose(grad_x, np.array([[1.],\n",
    "                                     [1.],\n",
    "                                     [3.]], dtype=np.float32)), \"Derivative error in first argument of Maximum\"\n",
    "assert np.allclose(grad_y, np.array([2., 2., 0.], dtype=np.float32)\n",
    "                   ), \"Derivative error in first argument of Maximum\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce Operations\n",
    "\n",
    "Reduce operations apply a function on an axis and the result of these operators decreases the dimension of the axis that they operate. Let's complete Reduce operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Complete ```ReduceOperations``` in the ```operations.py``` script.\n",
    "\n",
    "> Complete ```Sum``` in the ```operations.py``` script.\n",
    "\n",
    "> Complete ```Mean``` in the ```operations.py``` script.\n",
    "\n",
    "> Complete ```Max``` in the ```operations.py``` script.\n",
    "\n",
    "Test ```Sum``` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autograd.array import Array\n",
    "\n",
    "x_arr = Array(np.arange(18, dtype=np.float32).reshape((2, 3, 3)))\n",
    "z_arr = x_arr.sum(axis=1)\n",
    "assert np.allclose(z_arr.value,  np.array([[9., 12., 15.],\n",
    "                                           [36., 39., 42.]], dtype=np.float32)), \"Forward error in Sum\"\n",
    "grad_x = z_arr.operation.vjp(np.ones_like(z_arr.value))\n",
    "assert np.allclose(grad_x, np.ones_like(x_arr.value)), \"Derivative error in first argument of Sum\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Mean``` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_arr = x_arr.mean(axis=1, keepdims=True)\n",
    "assert np.allclose(z_arr.value,  np.array([[[3., 4., 5.]],\n",
    "                                           [[12., 13., 14.]]], dtype=np.float32)), \"Forward error in Mean\"\n",
    "grad_x = z_arr.operation.vjp(np.ones_like(z_arr.value))\n",
    "assert np.allclose(grad_x, np.ones_like(x_arr.value)/3), \"Derivative error in first argument of Mean\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Max``` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXX Autograd array: [[[ 0.  1.  2.]\n",
      "  [ 3.  4.  5.]\n",
      "  [ 6.  7.  8.]]\n",
      "\n",
      " [[ 9. 10. 11.]\n",
      "  [12. 13. 14.]\n",
      "  [15. 16. 17.]]]\n"
     ]
    }
   ],
   "source": [
    "z_arr = x_arr.max(axis=1)\n",
    "assert np.allclose(z_arr.value,  np.array([[6., 7., 8.],\n",
    "                                           [15., 16., 17.]], dtype=np.float32)), \"Forward error in Max\"\n",
    "print(\"XXXX\", x_arr)\n",
    "grad_x = z_arr.operation.vjp(np.ones_like(z_arr.value))\n",
    "true_grad = np.zeros_like(x_arr.value)\n",
    "true_grad[:, 2, :] = 1.0\n",
    "assert np.allclose(grad_x, true_grad), \"Derivative error in first argument of Max\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot product\n",
    "\n",
    "Now, we need to implement matrix multiplication operation.\n",
    "\n",
    "> Complete ```Matmul``` in the ```operations.py``` script.\n",
    "\n",
    "Test ```Matmul``` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autograd.array import Array\n",
    "\n",
    "x_arr = Array(np.arange(6, dtype=np.float32).reshape((2, 3)))\n",
    "y_arr = Array(np.arange(6, 18, dtype=np.float32).reshape((3, 4)))\n",
    "z_arr = x_arr @ y_arr\n",
    "assert np.allclose(z_arr.value,  np.array([[38.,  41.,  44.,  47.],\n",
    "                                           [128., 140., 152., 164.]], dtype=np.float32)), \"Forward error in Matmul\"\n",
    "grad_x, grad_y = z_arr.operation.vjp(np.ones_like(z_arr.value))\n",
    "assert np.allclose(grad_x, np.array([[30., 46., 62.],\n",
    "                                     [30., 46., 62.]], dtype=np.float32)), \"Derivative error in first argument of Matmul\"\n",
    "assert np.allclose(grad_y, np.array([[3., 3., 3., 3.],\n",
    "                                     [5., 5., 5., 5.],\n",
    "                                     [7., 7., 7., 7.]], dtype=np.float32)), \"Derivative error in first argument of Matmul\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous\n",
    "\n",
    "Finally, we need to complete some basic operations.\n",
    "\n",
    "> Complete ```Tanh``` Operation in ```operations.py``` (See the Sigmoid Operation)\n",
    "\n",
    "> Complete ```Exp``` Operation in ```operations.py```\n",
    "\n",
    "> Complete ```Pow``` Operation in ```operations.py```\n",
    "\n",
    "> Complete ```Log``` Operation in ```operations.py```\n",
    "\n",
    "> Complete ```Onehot``` Operation in ```operations.py```\n",
    "\n",
    "Please take a look at ```Array``` class and how we use these operations to better understand the structure of autograd.\n",
    "\n",
    "Let's test ```Tanh``` Operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENTS defaultdict(<class 'list'>, {Autograd array, Operation: Tanh: [[-0.9640276  -0.83365464 -0.37994897]\n",
      " [ 0.37994897  0.83365464  0.9640276 ]]: [array([[1., 1., 1.],\n",
      "       [1., 1., 1.]], dtype=float32)]})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from autograd.array import Array\n",
    "from autograd import grad\n",
    "\n",
    "x_arr = Array(np.linspace(-2, 2, 6).astype(np.float32).reshape((2, 3)), is_parameter=True)\n",
    "y_arr = x_arr.tanh()  # Using Array's function (see Array's tanh function)\n",
    "\n",
    "assert np.allclose(y_arr.value, np.array([[-0.9640276,  -0.83365464, -0.37994897],\n",
    "                                          [0.37994897,  0.83365464,  0.9640276]], dtype=np.float32))\n",
    "dx = grad(y_arr, gradient_vector=np.ones_like(y_arr.value))[x_arr]\n",
    "assert np.allclose(dx, np.array([[0.07065082, 0.30501992, 0.85563874],\n",
    "                                 [0.85563874, 0.30501992, 0.07065082]], dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Exp``` Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENTS defaultdict(<class 'list'>, {Autograd array, Operation: Exp: [[0.13533528 0.3011942  0.67032003]\n",
      " [1.4918246  3.3201172  7.3890557 ]]: [array([[1., 1., 1.],\n",
      "       [1., 1., 1.]], dtype=float32)]})\n"
     ]
    }
   ],
   "source": [
    "x_arr = Array(np.linspace(-2, 2, 6).astype(np.float32).reshape((2, 3)), is_parameter=True)\n",
    "y_arr = x_arr.exp()\n",
    "\n",
    "assert np.allclose(y_arr.value, grad(y_arr, np.ones_like(y_arr.value))[x_arr])\n",
    "assert np.allclose(y_arr.value, np.array([[0.13533528, 0.3011942 , 0.67032003],\n",
    "                                    [1.4918246 , 3.3201172 , 7.3890557 ]], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Pow``` Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENTS defaultdict(<class 'list'>, {Autograd array, Operation: Power: [[4.         1.44       0.16000001]\n",
      " [0.16000001 1.44       4.        ]]: [array([[1., 1., 1.],\n",
      "       [1., 1., 1.]], dtype=float32)]})\n"
     ]
    }
   ],
   "source": [
    "x_arr = Array(np.linspace(-2, 2, 6).astype(np.float32).reshape((2, 3)), is_parameter=True)\n",
    "y_arr = x_arr ** 2\n",
    "\n",
    "assert np.allclose(y_arr.value, np.array([[4.0000, 1.4400, 0.1600],\n",
    "                                          [0.1600, 1.4400, 4.0000]], dtype=np.float32))\n",
    "assert np.allclose(grad(y_arr, np.ones_like(y_arr.value))[x_arr], np.array([[-4.0000, -2.4000, -0.8000],\n",
    "                                                                  [0.8000,  2.4000,  4.0000]], dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Log``` Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = Array(np.linspace(0.1, 2, 6, dtype=np.float32).reshape(2, 3), is_parameter=True)\n",
    "y_arr = x_arr.log()\n",
    "\n",
    "assert np.allclose(y_arr.value, np.array([[-2.3025851, -0.7339692, -0.15082288],\n",
    "                                          [0.21511139,  0.48242617,  0.6931472]], dtype=np.float32))\n",
    "\n",
    "\n",
    "#assert np.allclose(grad(y_arr.sum().sum())[x_arr], np.array([[10.0000,  2.0833,  1.1628], \n",
    "                                                             #[0.8065,  0.6173,  0.5000]], dtype=np.float32), atol=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```Onehot``` Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENTS defaultdict(<class 'list'>, {Autograd array, Operation: Onehot: [[0. 1. 0.]\n",
      " [0. 0. 1.]]: [array([[0., 1., 2.],\n",
      "       [3., 4., 5.]], dtype=float32)]})\n"
     ]
    }
   ],
   "source": [
    "x_arr = Array(np.array([1, 2]), is_parameter=True)\n",
    "y_arr = x_arr.onehot(3)\n",
    "\n",
    "assert np.allclose(y_arr.value, np.array([[0, 1, 0],[0, 0, 1]], dtype=np.float32))\n",
    "assert np.allclose(grad(y_arr, np.arange(6).reshape(2, 3).astype(np.float32))[x_arr], np.array([1, 5.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions using Array Operations\n",
    "\n",
    "We implemented basic operations for building sophisticated neural network layers and functions. For example, we can implement ```relu``` function.\n",
    "\n",
    "```Python\n",
    "def relu(array: Array) -> Array:\n",
    "    return array.maximum(0.0)\n",
    "```\n",
    "\n",
    "Since we have already implemented the ```maximum``` operation, we do not need to deal with the gradient of the ```relu``` function. We can use the ```relu``` function to build neural networks and its ```vjp``` will be automatically called during gradient calculations. Similarly, we can implement other functions using ```Array``` operations that we already defined.\n",
    "\n",
    "> Complete ```leaky_relu``` in ```functions.py``` using ```Array``` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENTS defaultdict(<class 'list'>, {Autograd array, Operation: Sum: [-0.2 -0.1  0.1  1.   2. ]: [array(1.)]})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from autograd.functions import leaky_relu\n",
    "from autograd.array import Array\n",
    "from autograd import grad\n",
    "\n",
    "x_arr = Array(np.array([-2, -1, .10, 1, 2], dtype=np.float32), is_parameter=True)\n",
    "y_arr = leaky_relu(x_arr, negative_slope=0.1)\n",
    "assert np.allclose(grad(y_arr.sum(axis=0))[x_arr], np.array([0.1, 0.1, 1, 1, 1], dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Complete ```nll_with_logits_loss``` in ```functions.py``` using ```Array``` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENTS defaultdict(<class 'list'>, {Autograd array, Operation: Mean: 0.6161350011825562: [array(1.)]})\n",
      "VAR OPERATION <autograd.operations.Mean object at 0x000001E63C3D36C8>\n",
      "VAR OPERATION <autograd.operations.Mean object at 0x000001E63C3D36C8>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3BC8>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3BC8>\n",
      "VAR OPERATION None\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63C3D3C88>\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63C3D3C88>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3A48>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3A48>\n",
      "VAR OPERATION <autograd.operations.Onehot object at 0x000001E63C3D3E88>\n",
      "VAR OPERATION <autograd.operations.Onehot object at 0x000001E63C3D3E88>\n",
      "VAR OPERATION None\n",
      "VAR OPERATION <autograd.operations.Log object at 0x000001E63C3D3CC8>\n",
      "VAR OPERATION <autograd.operations.Log object at 0x000001E63C3D3CC8>\n",
      "VAR OPERATION <autograd.operations.Divide object at 0x000001E63A2F1908>\n",
      "VAR OPERATION <autograd.operations.Divide object at 0x000001E63A2F1908>\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63A2F1548>\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63A2F1548>\n",
      "VAR OPERATION <autograd.operations.Exp object at 0x000001E63C3D7048>\n",
      "VAR OPERATION <autograd.operations.Exp object at 0x000001E63C3D7048>\n",
      "VAR OPERATION None\n",
      "GRADIENTS defaultdict(<class 'list'>, {Autograd array, Operation: Mean: 0.6161350011825562: [array(1.)]})\n",
      "VAR OPERATION <autograd.operations.Mean object at 0x000001E63C3D36C8>\n",
      "VAR OPERATION <autograd.operations.Mean object at 0x000001E63C3D36C8>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3BC8>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3BC8>\n",
      "VAR OPERATION None\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63C3D3C88>\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63C3D3C88>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3A48>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3A48>\n",
      "VAR OPERATION <autograd.operations.Onehot object at 0x000001E63C3D3E88>\n",
      "VAR OPERATION <autograd.operations.Onehot object at 0x000001E63C3D3E88>\n",
      "VAR OPERATION None\n",
      "VAR OPERATION <autograd.operations.Log object at 0x000001E63C3D3CC8>\n",
      "VAR OPERATION <autograd.operations.Log object at 0x000001E63C3D3CC8>\n",
      "VAR OPERATION <autograd.operations.Divide object at 0x000001E63A2F1908>\n",
      "VAR OPERATION <autograd.operations.Divide object at 0x000001E63A2F1908>\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63A2F1548>\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63A2F1548>\n",
      "VAR OPERATION <autograd.operations.Exp object at 0x000001E63C3D7048>\n",
      "VAR OPERATION <autograd.operations.Exp object at 0x000001E63C3D7048>\n",
      "VAR OPERATION None\n",
      "{Autograd array: [[ 0.3 -0.4  0.1  1.3]]: array([[ 0.19866505,  0.09865414,  0.1626532 , -0.45997238]])}\n",
      "GRADIENTS defaultdict(<class 'list'>, {Autograd array, Operation: Mean: 0.6161350011825562: [array(1.)]})\n",
      "VAR OPERATION <autograd.operations.Mean object at 0x000001E63C3D36C8>\n",
      "VAR OPERATION <autograd.operations.Mean object at 0x000001E63C3D36C8>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3BC8>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3BC8>\n",
      "VAR OPERATION None\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63C3D3C88>\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63C3D3C88>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3A48>\n",
      "VAR OPERATION <autograd.operations.Multiply object at 0x000001E63C3D3A48>\n",
      "VAR OPERATION <autograd.operations.Onehot object at 0x000001E63C3D3E88>\n",
      "VAR OPERATION <autograd.operations.Onehot object at 0x000001E63C3D3E88>\n",
      "VAR OPERATION None\n",
      "VAR OPERATION <autograd.operations.Log object at 0x000001E63C3D3CC8>\n",
      "VAR OPERATION <autograd.operations.Log object at 0x000001E63C3D3CC8>\n",
      "VAR OPERATION <autograd.operations.Divide object at 0x000001E63A2F1908>\n",
      "VAR OPERATION <autograd.operations.Divide object at 0x000001E63A2F1908>\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63A2F1548>\n",
      "VAR OPERATION <autograd.operations.Sum object at 0x000001E63A2F1548>\n",
      "VAR OPERATION <autograd.operations.Exp object at 0x000001E63C3D7048>\n",
      "VAR OPERATION <autograd.operations.Exp object at 0x000001E63C3D7048>\n",
      "VAR OPERATION None\n"
     ]
    }
   ],
   "source": [
    "from autograd.functions import nll_with_logits_loss\n",
    "\n",
    "logits = Array(np.array([[0.3, -0.4, 0.1, 1.3]], dtype=np.float32), is_parameter=True)\n",
    "label = Array(np.array([3], dtype=np.int64))\n",
    "\n",
    "\n",
    "loss = nll_with_logits_loss(logits, label).mean()\n",
    "\n",
    "assert np.allclose(loss.value, np.array([0.616135], dtype=np.float32)), \"Error in evaluation\"\n",
    "\n",
    "a = grad(loss)[logits]\n",
    "print(grad(loss))\n",
    "assert np.allclose(grad(loss)[logits], np.array([[ 0.19866505,  0.09865415,  0.1626532 , -0.4599724 ]])), \"Error in gradient\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can continue with neural networks using all the functions we have implemented so far."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ee2ecf61b74c3b0e997a17b9194eac603566e9117375fa5485c0b29d12eba50"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
